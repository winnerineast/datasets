<div itemscope itemtype="http://schema.org/Dataset">
  <div itemscope itemprop="includedInDataCatalog" itemtype="http://schema.org/DataCatalog">
    <meta itemprop="name" content="TensorFlow Datasets" />
  </div>
  <meta itemprop="name" content="c4" />
  <meta itemprop="description" content="A colossal, cleaned version of Common Crawl's web crawl corpus.&#10;&#10;Based on Common Crawl dataset: &quot;https://commoncrawl.org&quot;&#10;&#10;Due to the overhead of cleaning the dataset, it is recommend you prepare it with&#10;a distributed service like Cloud Dataflow. More info at&#10;https://www.tensorflow.org/datasets/beam_datasets.&#10;&#10;&#10;To use this dataset:&#10;&#10;```python&#10;import tensorflow_datasets as tfds&#10;&#10;ds = tfds.load('c4', split='train')&#10;for ex in ds.take(4):&#10;  print(ex)&#10;```&#10;&#10;See [the guide](https://www.tensorflow.org/datasets/overview) for more&#10;informations on [tensorflow_datasets](https://www.tensorflow.org/datasets).&#10;&#10;" />
  <meta itemprop="url" content="https://www.tensorflow.org/datasets/catalog/c4" />
  <meta itemprop="sameAs" content="https://github.com/google-research/text-to-text-transfer-transformer#datasets" />
  <meta itemprop="citation" content="&#10;@article{2019t5,&#10;  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},&#10;  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},&#10;  journal = {arXiv e-prints},&#10;  year = {2019},&#10;  archivePrefix = {arXiv},&#10;  eprint = {1910.10683},&#10;}&#10;" />
</div>
# `c4` (Manual download)

A colossal, cleaned version of Common Crawl's web crawl corpus.

Based on Common Crawl dataset: "https://commoncrawl.org"

Due to the overhead of cleaning the dataset, it is recommend you prepare it with
a distributed service like Cloud Dataflow. More info at
https://www.tensorflow.org/datasets/beam_datasets.

*   URL:
    [https://github.com/google-research/text-to-text-transfer-transformer#datasets](https://github.com/google-research/text-to-text-transfer-transformer#datasets)
*   `DatasetBuilder`:
    [`tfds.text.c4.C4`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/text/c4.py)

`c4` is configured with `tfds.text.c4.C4Config` and has the following
configurations predefined (defaults to the first one):

*   `en` (`v1.1.0`) (`Size: 6.96 TiB`): English C4 dataset.

*   `en.noclean` (`v1.1.0`) (`Size: 6.96 TiB`): Disables all cleaning
    (deduplication, removal based on bad words, etc.)

*   `en.realnewslike` (`v1.1.0`) (`Size: 6.96 TiB`): Filters from the default
    config to only include content from the domains used in the 'RealNews'
    dataset (Zellers et al., 2019).

*   `en.webtextlike` (`v1.1.0`) (`Size: 6.96 TiB`): Filters from the default
    config to only include content from the URLs in OpenWebText
    (https://github.com/jcpeterson/openwebtext).

## `c4/en`
English C4 dataset.

Versions:

*   **`1.1.0`** (default):
*   `2.0.0`: New split API (https://tensorflow.org/datasets/splits)
*   `1.0.1`: None
*   `1.0.0`: None

WARNING: This dataset requires you to download the source data manually into
manual_dir (defaults to `~/tensorflow_datasets/manual/c4/`): For the
WebText-like config, you must manually download 'OpenWebText.zip' (from
https://mega.nz/#F!EZZD0YwJ!9_PlEQzdMVLaNdKv_ICNVQ) and the Common Crawl WET
files from August 2018 to July 2019
(https://commoncrawl.org/the-data/get-started/) and place them in the
`manual_dir`.

### Statistics
None computed

### Features
```python
FeaturesDict({
    'content-length': Text(shape=(), dtype=tf.string),
    'content-type': Text(shape=(), dtype=tf.string),
    'text': Text(shape=(), dtype=tf.string),
    'timestamp': Text(shape=(), dtype=tf.string),
    'url': Text(shape=(), dtype=tf.string),
})
```

### Homepage

*   [https://github.com/google-research/text-to-text-transfer-transformer#datasets](https://github.com/google-research/text-to-text-transfer-transformer#datasets)

## `c4/en.noclean`
Disables all cleaning (deduplication, removal based on bad words, etc.)

Versions:

*   **`1.1.0`** (default):
*   `2.0.0`: New split API (https://tensorflow.org/datasets/splits)
*   `1.0.1`: None
*   `1.0.0`: None

WARNING: This dataset requires you to download the source data manually into
manual_dir (defaults to `~/tensorflow_datasets/manual/c4/`): For the
WebText-like config, you must manually download 'OpenWebText.zip' (from
https://mega.nz/#F!EZZD0YwJ!9_PlEQzdMVLaNdKv_ICNVQ) and the Common Crawl WET
files from August 2018 to July 2019
(https://commoncrawl.org/the-data/get-started/) and place them in the
`manual_dir`.

### Statistics
None computed

### Features
```python
FeaturesDict({
    'content-length': Text(shape=(), dtype=tf.string),
    'content-type': Text(shape=(), dtype=tf.string),
    'text': Text(shape=(), dtype=tf.string),
    'timestamp': Text(shape=(), dtype=tf.string),
    'url': Text(shape=(), dtype=tf.string),
})
```

### Homepage

*   [https://github.com/google-research/text-to-text-transfer-transformer#datasets](https://github.com/google-research/text-to-text-transfer-transformer#datasets)

## `c4/en.realnewslike`

Filters from the default config to only include content from the domains used in
the 'RealNews' dataset (Zellers et al., 2019).

Versions:

*   **`1.1.0`** (default):
*   `2.0.0`: New split API (https://tensorflow.org/datasets/splits)
*   `1.0.1`: None
*   `1.0.0`: None

WARNING: This dataset requires you to download the source data manually into
manual_dir (defaults to `~/tensorflow_datasets/manual/c4/`): For the
WebText-like config, you must manually download 'OpenWebText.zip' (from
https://mega.nz/#F!EZZD0YwJ!9_PlEQzdMVLaNdKv_ICNVQ) and the Common Crawl WET
files from August 2018 to July 2019
(https://commoncrawl.org/the-data/get-started/) and place them in the
`manual_dir`.

### Statistics
None computed

### Features
```python
FeaturesDict({
    'content-length': Text(shape=(), dtype=tf.string),
    'content-type': Text(shape=(), dtype=tf.string),
    'text': Text(shape=(), dtype=tf.string),
    'timestamp': Text(shape=(), dtype=tf.string),
    'url': Text(shape=(), dtype=tf.string),
})
```

### Homepage

*   [https://github.com/google-research/text-to-text-transfer-transformer#datasets](https://github.com/google-research/text-to-text-transfer-transformer#datasets)

## `c4/en.webtextlike`

Filters from the default config to only include content from the URLs in
OpenWebText (https://github.com/jcpeterson/openwebtext).

Versions:

*   **`1.1.0`** (default):
*   `2.0.0`: New split API (https://tensorflow.org/datasets/splits)
*   `1.0.1`: None
*   `1.0.0`: None

WARNING: This dataset requires you to download the source data manually into
manual_dir (defaults to `~/tensorflow_datasets/manual/c4/`): For the
WebText-like config, you must manually download 'OpenWebText.zip' (from
https://mega.nz/#F!EZZD0YwJ!9_PlEQzdMVLaNdKv_ICNVQ) and the Common Crawl WET
files from August 2018 to July 2019
(https://commoncrawl.org/the-data/get-started/) and place them in the
`manual_dir`.

### Statistics
None computed

### Features
```python
FeaturesDict({
    'content-length': Text(shape=(), dtype=tf.string),
    'content-type': Text(shape=(), dtype=tf.string),
    'text': Text(shape=(), dtype=tf.string),
    'timestamp': Text(shape=(), dtype=tf.string),
    'url': Text(shape=(), dtype=tf.string),
})
```

### Homepage

*   [https://github.com/google-research/text-to-text-transfer-transformer#datasets](https://github.com/google-research/text-to-text-transfer-transformer#datasets)

## Citation
```
@article{2019t5,
  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {arXiv e-prints},
  year = {2019},
  archivePrefix = {arXiv},
  eprint = {1910.10683},
}
```

--------------------------------------------------------------------------------
